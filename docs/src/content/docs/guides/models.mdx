---
title: Models
description: Choose and configure language models for your agents
---

import { Code } from '@astrojs/starlight/components';
import modelCustomProviderExample from '../../../../../examples/docs/models/customProviders.ts?raw';
import setDefaultOpenAIKeyExample from '../../../../../examples/docs/config/setDefaultOpenAIKey.ts?raw';
import modelSettingsExample from '../../../../../examples/docs/models/modelSettings.ts?raw';
import promptIdExample from '../../../../../examples/basic/prompt-id.ts?raw';
import agentWithModelExample from '../../../../../examples/docs/models/agentWithModel.ts?raw';
import runnerWithModelExample from '../../../../../examples/docs/models/runnerWithModel.ts?raw';
import setTracingExportApiKeyExample from '../../../../../examples/docs/config/setTracingExportApiKey.ts?raw';

Every Agent ultimately calls an LLM. The SDK abstracts models behind two lightweight
interfaces:

- [`Model`](/openai-agents-js/openai/agents/interfaces/model) – knows how to make _one_ request against a
  specific API.
- [`ModelProvider`](/openai-agents-js/openai/agents/interfaces/modelprovider) – resolves human‑readable
  model **names** (e.g. `'gpt‑4o'`) to `Model` instances.

In day‑to‑day work you normally only interact with model **names** and occasionally
`ModelSettings`.

<Code
  lang="typescript"
  code={agentWithModelExample}
  title="Specifying a model per‑agent"
/>

---

## The OpenAI provider

The default `ModelProvider` resolves names using the OpenAI APIs. It supports two distinct
endpoints:

| API              | Usage                                                             | Call `setOpenAIAPI()`                   |
| ---------------- | ----------------------------------------------------------------- | --------------------------------------- |
| Chat Completions | Standard chat & function calls                                    | `setOpenAIAPI('chat_completions')`      |
| Responses        | New streaming‑first generative API (tool calls, flexible outputs) | `setOpenAIAPI('responses')` _(default)_ |

### Authentication

<Code
  lang="typescript"
  code={setDefaultOpenAIKeyExample}
  title="Set default OpenAI key"
/>

You can also plug your own `OpenAI` client via `setDefaultOpenAIClient(client)` if you need
custom networking settings.

### Default model

The OpenAI provider defaults to `gpt‑4o`. Override per agent or globally:

<Code
  lang="typescript"
  code={runnerWithModelExample}
  title="Set a default model"
/>

---

## ModelSettings

`ModelSettings` mirrors the OpenAI parameters but is provider‑agnostic.

| Field               | Type                                       | Notes                                                                     |
| ------------------- | ------------------------------------------ | ------------------------------------------------------------------------- |
| `temperature`       | `number`                                   | Creativity vs. determinism.                                               |
| `topP`              | `number`                                   | Nucleus sampling.                                                         |
| `frequencyPenalty`  | `number`                                   | Penalise repeated tokens.                                                 |
| `presencePenalty`   | `number`                                   | Encourage new tokens.                                                     |
| `toolChoice`        | `'auto' \| 'required' \| 'none' \| string` | See [forcing tool use](/openai-agents-js/guides/agents#forcing-tool-use). |
| `parallelToolCalls` | `boolean`                                  | Allow parallel function calls where supported.                            |
| `truncation`        | `'auto' \| 'disabled'`                     | Token truncation strategy.                                                |
| `maxTokens`         | `number`                                   | Maximum tokens in the response.                                           |
| `store`             | `boolean`                                  | Persist the response for retrieval / RAG workflows.                       |

Attach settings at either level:

<Code lang="typescript" code={modelSettingsExample} title="Model settings" />

`Runner`‑level settings override any conflicting per‑agent settings.

---

## Prompt

Agents can be configured with a `prompt` parameter, indicating a server-stored
prompt configuration that should be used to control the Agent's behavior. Currently,
this option is only supported when you use the OpenAI
[Responses API](https://platform.openai.com/docs/api-reference/responses).

| Field       | Type     | Notes                                                                                                                                  |
| ----------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| `promptId`  | `string` | Unique identifier for a prompt.                                                                                                        |
| `version`   | `string` | Version of the prompt you wish to use.                                                                                                 |
| `variables` | `object` | A key/value pair of variables to substitute into the prompt. Values can be strings or content input types like text, images, or files. |

<Code lang="typescript" code={promptIdExample} title="Agent with prompt" />

Any additional agent configuration, like tools or instructions, will override the
values you may have configured in your stored prompt.

---

## Custom model providers

Implementing your own provider is straightforward – implement `ModelProvider` and `Model` and
pass the provider to the `Runner` constructor:

<Code
  lang="typescript"
  code={modelCustomProviderExample}
  title="Minimal custom provider"
/>

---

## Tracing exporter

When using the OpenAI provider you can opt‑in to automatic trace export by providing your API
key:

<Code
  lang="typescript"
  code={setTracingExportApiKeyExample}
  title="Tracing exporter"
/>

This sends traces to the [OpenAI dashboard](https://platform.openai.com/traces) where you can
inspect the complete execution graph of your workflow.

---

## Next steps

- Explore [running agents](/openai-agents-js/guides/running-agents).
- Give your models super‑powers with [tools](/openai-agents-js/guides/tools).
- Add [guardrails](/openai-agents-js/guides/guardrails) or [tracing](/openai-agents-js/guides/tracing) as needed.
