---
title: 모델 컨텍스트 프로토콜 (MCP)
description: Learn how to utilize MCP servers as tools
---

import { Code } from '@astrojs/starlight/components';
import hostedAgentExample from '../../../../../../examples/docs/mcp/hostedAgent.ts?raw';
import hostedExample from '../../../../../../examples/docs/mcp/hosted.ts?raw';
import hostedStreamExample from '../../../../../../examples/docs/mcp/hostedStream.ts?raw';
import hostedHITLExample from '../../../../../../examples/docs/mcp/hostedHITL.ts?raw';
import streamableHttpExample from '../../../../../../examples/docs/mcp/streamableHttp.ts?raw';
import stdioExample from '../../../../../../examples/docs/mcp/stdio.ts?raw';
import toolFilterExample from '../../../../../../examples/docs/mcp/tool-filter.ts?raw';

The [**Model Context Protocol (MCP)**](https://modelcontextprotocol.io) is an open protocol that standardizes how applications provide tools and context to LLMs. From the MCP docs:

> MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.

이 SDK가 지원하는 MCP 서버 유형은 세 가지입니다:

1. **호스티드 MCP 서버 도구** – [OpenAI Responses API](https://platform.openai.com/docs/guides/tools-remote-mcp)가 도구로 사용하는 원격 MCP 서버
2. **Streamable HTTP MCP 서버** – [Streamable HTTP transport](https://modelcontextprotocol.io/docs/concepts/transports#streamable-http)를 구현한 로컬 또는 원격 서버
3. **Stdio MCP 서버** – 표준 입출력으로 접근하는 서버(가장 간단한 옵션)

사용 사례에 따라 서버 유형을 선택합니다:

| 필요한 것                                                            | 권장 옵션                |
| -------------------------------------------------------------------- | ------------------------ |
| 기본 OpenAI Responses 모델로 공개 접근 가능한 원격 서버 호출         | **1. 호스티드 MCP 도구** |
| 공개 접근 가능한 원격 서버를 사용하되 도구 호출은 로컬에서 트리거    | **2. Streamable HTTP**   |
| 로컬에서 실행 중인 Streamable HTTP 서버 사용                         | **2. Streamable HTTP**   |
| OpenAI Responses가 아닌 모델과 함께 임의의 Streamable HTTP 서버 사용 | **2. Streamable HTTP**   |
| 표준 I/O 프로토콜만 지원하는 로컬 MCP 서버 사용                      | **3. Stdio**             |

## 1. 호스티드 MCP 서버 도구

호스티드 도구는 전체 왕복 처리(round-trip)를 모델 내부에서 수행합니다. 애플리케이션 코드가 MCP 서버를 호출하는 대신, OpenAI Responses API가 원격 도구 엔드포인트를 호출하고 결과를 모델로 스트리밍합니다.

다음은 호스티드 MCP 서버 도구를 사용하는 가장 간단한 예시입니다. 원격 MCP 서버의 레이블과 URL을 `hostedMcpTool` 유틸리티 함수에 전달하여 호스티드 MCP 서버 도구를 만들 수 있습니다.

<Code lang="typescript" code={hostedAgentExample} title="hostedAgent.ts" />

그런 다음 `run` 함수(또는 사용자 정의한 `Runner` 인스턴스의 `run` 메서드)로 에이전트를 실행할 수 있습니다:

<Code
  lang="typescript"
  code={hostedExample}
  title="Run with hosted MCP tools"
/>

증분 MCP 결과를 스트리밍하려면, 에이전트를 실행할 때 `stream: true`를 전달합니다:

<Code
  lang="typescript"
  code={hostedStreamExample}
  title="Run with hosted MCP tools (streaming)"
/>

#### 선택적 승인 흐름

민감한 작업의 경우, 개별 도구 호출에 대해 사람의 승인을 요구할 수 있습니다. `requireApproval: 'always'` 또는 도구 이름을 `'never'`/`'always'`에 매핑하는 세부 설정 객체를 전달합니다.

프로그래밍적으로 도구 호출의 안전성을 판단할 수 있다면, [`onApproval` 콜백](https://github.com/openai/openai-agents-js/blob/main/examples/mcp/hosted-mcp-on-approval.ts)을 사용해 도구 호출을 승인하거나 거부할 수 있습니다. 사람의 승인이 필요한 경우, 로컬 함수 도구에서와 동일하게 `interruptions`를 사용하는 [휴먼인더루프 (HITL)](/openai-agents-js/ko/guides/human-in-the-loop/) 접근 방식을 사용할 수 있습니다.

완전하게 동작하는 샘플 코드(호스티드 도구/Streamable HTTP/stdio + 스트리밍, 휴먼인더루프 (HITL), onApproval)는 GitHub 리포지토리의 [examples/mcp](https://github.com/openai/openai-agents-js/tree/main/examples/mcp)에 있습니다.

## 2. Streamable HTTP MCP 서버

에이전트가 로컬 또는 원격의 Streamable HTTP MCP 서버와 직접 통신하는 경우, 서버의 `url`, `name` 및 선택적 설정과 함께 `MCPServerStreamableHttp`를 인스턴스화합니다:

<Code
  lang="typescript"
  code={streamableHttpExample}
  title="Run with Streamable HTTP MCP servers"
/>

생성자는 `authProvider`, `requestInit`, `fetch`, `reconnectionOptions`, `sessionId`와 같은 추가 MCP TypeScript SDK 옵션도 받습니다. 자세한 내용은 [MCP TypeScript SDK repository](https://github.com/modelcontextprotocol/typescript-sdk)와 관련 문서를 참조하세요.

## 3. Stdio MCP 서버

표준 입출력만 노출하는 서버의 경우, `fullCommand`로 `MCPServerStdio`를 인스턴스화합니다:

<Code
  lang="typescript"
  code={stdioExample}
  title="Run with Stdio MCP servers"
/>

## 알아둘 기타 사항

**Streamable HTTP** 및 **Stdio** 서버의 경우, 에이전트가 실행될 때마다 사용 가능한 도구를 파악하기 위해 `list_tools()`를 호출할 수 있습니다. 이 왕복 호출은 지연 시간을 증가시킬 수 있으므로—특히 원격 서버의 경우—`MCPServerStdio` 또는 `MCPServerStreamableHttp`에 `cacheToolsList: true`를 전달해 결과를 메모리에 캐시할 수 있습니다.

도구 목록이 변경되지 않음을 확신하는 경우에만 활성화하세요. 이후 캐시를 무효화하려면 서버 인스턴스에서 `invalidateToolsCache()`를 호출합니다.

### 도구 필터링

`createMCPToolStaticFilter`를 통해 정적 필터를 전달하거나, 사용자 정의 함수를 전달해 각 서버에서 노출할 도구를 제한할 수 있습니다. 다음은 두 접근 방식을 함께 보여주는 예시입니다:

<Code lang="typescript" code={toolFilterExample} title="Tool filtering" />

## 추가 자료

- [Model Context Protocol](https://modelcontextprotocol.io/) – 공식 명세
- [examples/mcp](https://github.com/openai/openai-agents-js/tree/main/examples/mcp) – 위에서 언급한 실행 가능한 데모
