---
title: 컨텍스트 관리
description: Learn how to provide local data via RunContext and expose context to the LLM
---

import { Aside, Code } from '@astrojs/starlight/components';
import localContextExample from '../../../../../../examples/docs/context/localContext.ts?raw';

컨텍스트는 문맥에 따라 여러 의미로 쓰입니다. 다음 두 가지 주요 컨텍스트 범주가 있습니다:

1. 실행 중 코드가 접근할 수 있는 **로컬 컨텍스트**: 도구에 필요한 의존성이나 데이터, `onHandoff` 같은 콜백, 라이프사이클 훅
2. 응답을 생성할 때 언어 모델이 볼 수 있는 **에이전트/LLM 컨텍스트**

## 로컬 컨텍스트

로컬 컨텍스트는 `RunContext<T>` 타입으로 표현됩니다. 상태나 의존성을 담을 임의의 객체를 만들고 이를 `Runner.run()`에 전달합니다. 모든 도구 호출과 훅은 `RunContext` 래퍼를 받아 해당 객체를 읽거나 수정할 수 있습니다.

<Code lang="typescript" code={localContextExample} title="로컬 컨텍스트 예제" />

단일 실행에 참여하는 모든 에이전트, 도구, 훅은 동일한 컨텍스트의 **type**을 사용해야 합니다.

로컬 컨텍스트는 다음과 같은 용도로 사용하세요:

- 실행에 대한 데이터(사용자 이름, ID 등)
- 로거나 데이터 패처 같은 의존성
- 헬퍼 함수

<Aside type="note">
  컨텍스트 객체는 LLM으로 **전송되지 않습니다**. 전적으로 로컬이므로 자유롭게
  읽고 쓸 수 있습니다.
</Aside>

## 에이전트/LLM 컨텍스트

LLM이 호출될 때 볼 수 있는 데이터는 대화 히스토리에서만 옵니다. 추가 정보를 제공하려면 다음 옵션을 사용할 수 있습니다:

1. 에이전트 `instructions`에 추가합니다. 시스템 또는 개발자 메시지로도 알려져 있습니다. 이는 정적인 문자열이거나 컨텍스트를 받아 문자열을 반환하는 함수일 수 있습니다.
2. `Runner.run()` 호출 시 `input`에 포함합니다. 이는 instructions 기법과 유사하지만, 메시지를 [명령 체계](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command)에서 더 낮은 위치에 둘 수 있습니다.
3. 함수 도구를 통해 노출하여 LLM이 필요할 때 데이터를 가져올 수 있게 합니다.
4. 리트리벌(retrieval) 또는 웹 검색 도구를 사용하여 파일, 데이터베이스, 웹의 관련 데이터에 근거해 응답을 생성합니다.
