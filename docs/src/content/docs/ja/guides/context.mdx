---
title: コンテキスト管理
description: Learn how to provide local data via RunContext and expose context to the LLM
---

import { Aside, Code } from '@astrojs/starlight/components';
import localContextExample from '../../../../../../examples/docs/context/localContext.ts?raw';

コンテキストという語は多義的です。気にすべき主なコンテキストには次の 2 つがあります:

1.  **ローカルコンテキスト** — 実行中にあなたのコードがアクセスできるもの。ツールに必要な依存関係やデータ、`onHandoff` のようなコールバック、ライフサイクルフック
2.  **エージェント/ LLM コンテキスト** — 言語モデルが応答を生成するときに参照できるもの

## ローカルコンテキスト

ローカルコンテキストは `RunContext<T>` 型で表現します。状態や依存関係を保持する任意のオブジェクトを作成し、`Runner.run()` に渡します。すべてのツール呼び出しやフックは `RunContext` ラッパーを受け取り、そのオブジェクトから読み取ったり、変更したりできます。

<Code
  lang="typescript"
  code={localContextExample}
  title="ローカルコンテキストの例"
/>

1 回の実行に参加するすべてのエージェント、ツール、フックは、同じ型のコンテキストを使う必要があります。

ローカルコンテキストの用途例:

- 実行に関するデータ（ユーザー名、ID など）
- ロガーやデータフェッチャーなどの依存関係
- ヘルパー関数

<Aside type="note">
  コンテキストオブジェクトは LLM に ** 送信されません **
  。純粋にローカルであり、自由に読み書きできます。
</Aside>

## エージェント/ LLM コンテキスト

LLM が呼び出されると、参照できるデータは会話履歴に由来するものだけです。追加情報を利用可能にする方法はいくつかあります:

1. エージェントの `instructions` に追加する — いわゆるシステムメッセージまたは開発者メッセージです。これは静的な文字列でも、コンテキストを受け取って文字列を返す関数でも構いません
2. `Runner.run()` を呼び出すときに `input` に含める。これは `instructions` を使う方法と似ていますが、[指揮系統](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command) のより下位にメッセージを配置できます
3. 関数ツールを通じて公開し、LLM がオンデマンドでデータを取得できるようにする
4. レトリーバルや Web 検索ツールを使い、ファイル、データベース、または Web からの関連データに基づいて応答をグラウンディングする
