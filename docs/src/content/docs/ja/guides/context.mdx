---
title: コンテキスト管理
description: Learn how to provide local data via RunContext and expose context to the LLM
---

import { Aside, Code } from '@astrojs/starlight/components';
import localContextExample from '../../../../../../examples/docs/context/localContext.ts?raw';

コンテキストという語は多義的です。気にすべきコンテキストには、大きく 2 つの種類があります:

1. **ローカルコンテキスト** – 実行中にコードがアクセスできる依存関係やツールが必要とするデータ、`onHandoff` のようなコールバック、ライフサイクルフックなど
2. **エージェント / LLM コンテキスト** – 言語モデルが応答を生成するときに参照できるもの

## ローカルコンテキスト

ローカルコンテキストは `RunContext<T>` 型で表されます。状態や依存関係を保持する任意のオブジェクトを作成し、それを `Runner.run()` に渡します。すべてのツール呼び出しとフックは `RunContext` ラッパーを受け取り、そのオブジェクトを読み書きできます。

<Code
  lang="typescript"
  code={localContextExample}
  title="Local context example"
/>

同一の実行に参加するすべてのエージェント、ツール、フックは同じ **型** のコンテキストを共有する必要があります。

ローカルコンテキストの主な用途:

- 実行に関するデータ（ユーザー名、ID など）
- ロガーやデータフェッチャーのような依存関係
- ヘルパー関数

<Aside type="note">
  コンテキストオブジェクトは **LLM
  に送信されません**。完全にローカルなもので、自由に読み書きできます。
</Aside>

## エージェント / LLM コンテキスト

LLM が呼び出されるとき、参照できるデータは会話履歴のみです。追加情報を渡すには、いくつか方法があります:

1. Agent の `instructions` に追加する – システムまたは developer メッセージとも呼ばれます。固定文字列でも、コンテキストを受け取って文字列を返す関数でもかまいません。
2. `Runner.run()` を呼び出す際に `input` に含める。この方法は instructions と似ていますが、[Chain-of-Command](https://cdn.openai.com/spec/model-spec-2024-05-08.html#follow-the-chain-of-command) でより下位のメッセージとして配置できます。
3. function tools 経由で公開し、 LLM が必要に応じてデータを取得できるようにする
4. retrieval ツールや Web 検索ツールを使い、ファイルやデータベース、Web から取得した関連データに基づいて回答を補強する
